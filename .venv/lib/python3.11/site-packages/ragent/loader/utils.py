import os
from datetime import datetime, timezone
from typing import Any, Dict, List, Type, Union

from langchain_core.document_loaders.base import BaseLoader

from ragent.loader.constants import CHUNK_OVERLAP, CHUNK_SIZE, LOADER_REGISTRY
from ragent.log_utils import logger

MAX_TOKEN_LIMIT = 8000


class InvalidLoaderType(Exception):
    """Custom exception for invalid loader types."""

    pass


def get_loader_class(loader_type: str) -> Type[BaseLoader]:
    """
    Get the loader class based on the loader type.

    Args:
        loader_type: Type of loader to use

    Returns:
        BaseLoader subclass

    Raises:
        InvalidLoaderType: If the loader type is not supported
    """
    loader_class = LOADER_REGISTRY.get(loader_type.lower())
    if not loader_class:
        raise InvalidLoaderType(f"Loader type '{loader_type}' is not supported")
    return loader_class


def detect_format(file_path: str) -> str:
    """
    Detect the format of a file based on its extension.

    Args:
        file_path: Path to the file

    Returns:
        Format string (e.g., 'pdf', 'csv')
    """
    return file_path.split(".")[-1].lower()


def is_loader_instance(obj: Any) -> bool:
    """
    Check if an object is a loader instance.

    Args:
        obj: Object to check

    Returns:
        bool: True if obj is a loader instance, False otherwise
    """
    return obj is not None and hasattr(obj, "load") and callable(obj.load)


def get_loader_instance_from_config(
    file_path: str,
    source_column: Union[str, None],
    loader_config: Union[str, BaseLoader, Dict, None],
    loader_params: Dict,
) -> BaseLoader:
    """
    Helper function to get a loader instance based on the configuration.

    Args:
        file_path: Path to the file to be loaded
        source_column: Column name to use as source (for CSV files)
        loader_config: Loader configuration
        loader_params: Additional parameters to pass to the loader

    Returns:
        A loader instance
    """
    # If a loader instance is provided, use it directly
    if is_loader_instance(loader_config):
        loader_instance = loader_config
        # If the loader instance doesn't have file_path set, try to set it
        if hasattr(loader_instance, "file_path") and not getattr(
            loader_instance, "file_path", None
        ):
            setattr(loader_instance, "file_path", file_path)
        return loader_instance

    # Handle dictionary mapping extensions to loaders
    if isinstance(loader_config, dict):
        return _get_loader_from_dict_config(
            file_path, source_column, loader_config, loader_params
        )

    # Handle string loader type
    if isinstance(loader_config, str):
        loader_class = get_loader_class(loader_config)
        params = _prepare_loader_params(
            file_path, source_column, loader_config, loader_params
        )
        return loader_class(**params)

    # Auto-detect loader from file extension
    format_type = detect_format(file_path)
    loader_class = get_loader_class(format_type)
    params = _prepare_loader_params(
        file_path, source_column, format_type, loader_params
    )
    return loader_class(**params)


def _get_loader_from_dict_config(
    file_path: str,
    source_column: Union[str, None],
    loader_config: Dict,
    loader_params: Dict,
) -> BaseLoader:
    """
    Get a loader instance from a dictionary configuration.

    Args:
        file_path: Path to the file to be loaded
        source_column: Column name to use as source (for CSV files)
        loader_config: Dictionary mapping extensions to loaders
        loader_params: Additional parameters to pass to the loader

    Returns:
        A loader instance
    """
    file_ext = os.path.splitext(file_path)[1].lower()

    # Look for exact match with extension (including dot)
    if file_ext in loader_config:
        loader_class = loader_config[file_ext]
    # Try without the dot
    elif file_ext[1:] in loader_config:
        loader_class = loader_config[file_ext[1:]]
    else:
        format_type = detect_format(file_path)
        loader_class = get_loader_class(format_type)

    # Configure parameters
    params = {"file_path": file_path}

    # Handle source_column for CSV
    if file_ext.lower() in (".csv", "csv") and source_column:
        params["source_column"] = source_column

    # Add any additional parameters
    params.update(loader_params)

    return loader_class(**params)


def _prepare_loader_params(
    file_path: str,
    source_column: Union[str, None],
    format_type: str,
    loader_params: Dict,
) -> Dict:
    """
    Prepare parameters for loader initialization.

    Args:
        file_path: Path to the file to be loaded
        source_column: Column name to use as source (for CSV files)
        format_type: Type of the file format
        loader_params: Additional parameters to pass to the loader

    Returns:
        Dictionary of parameters
    """
    params = {"file_path": file_path}

    if format_type.lower() == "csv" and source_column:
        params["source_column"] = source_column

    # Add any additional parameters
    if loader_params:
        params.update(loader_params)

    return params


def process_document_as_dict(
    documents: List,
    file_name: str,
    file_extension: str,
    chunk_size: int = CHUNK_SIZE,
    chunk_overlap: int = CHUNK_OVERLAP,
) -> Dict[str, Any]:
    """
    Process documents and create a content dictionary with metadata.

    Args:
        documents: List of loaded documents
        file_name: Name of the original file
        file_extension: Extension of the file (e.g., 'pdf', 'csv')
        chunk_size: Size of chunks used for processing
        chunk_overlap: Overlap between chunks

    Returns:
        Dict containing processed document chunks and metadata
    """
    current_time = datetime.now(timezone.utc).isoformat()
    content_dict = {}

    for idx, doc in enumerate(documents):
        chunk_id = f"chunk_{idx}"
        metadata = {
            "page_number": doc.metadata.get("page", 1),
            "source": file_name,
            "created_at": current_time,
        }

        # Add additional metadata from the document if available
        if hasattr(doc, "metadata"):
            metadata.update(
                {
                    k: v
                    for k, v in doc.metadata.items()
                    if k not in metadata and v is not None
                }
            )

        content = doc.page_content.strip()
        if not content:  # Skip empty chunks
            continue

        content_dict[chunk_id] = {"content": content, "metadata": metadata}

        logger.info(f"Processed chunk {chunk_id} with {len(content)} characters")

    # Add summary metadata
    content_dict["_metadata"] = {
        "num_chunks": len(
            content_dict
        ),  # This is the count of chunks without _metadata
        "file_type": file_extension,
        "processing_method": "langchain",
        "filename": file_name,
        "chunk_size": chunk_size,
        "chunk_overlap": chunk_overlap,
    }

    logger.info(
        f"Successfully processed {file_name} with {len(content_dict) - 1} chunks"
    )

    return content_dict
