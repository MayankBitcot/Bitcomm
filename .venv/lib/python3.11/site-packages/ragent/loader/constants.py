import json
from pathlib import Path
from typing import Any, Callable, Dict, Iterator, Optional, Type, Union

from langchain_community.document_loaders import (
    CSVLoader,
    Docx2txtLoader,
    PyMuPDFLoader,
    TextLoader,
    UnstructuredMarkdownLoader,
)
from langchain_community.document_loaders.base import BaseLoader
from langchain_core.documents import Document

from ragent.log_utils import logger

MAX_TOKEN_LIMIT = 4096  # Example limit for OpenAI tokens


class CustomJSONLoader(BaseLoader):
    """
    Load a `JSON` file using a `jq` schema, and process it into LangChain documents.

    This class will load a JSON file, process each item into a document,
    and handle oversize document checks.
    """

    def __init__(
        self,
        file_path: Union[str, Path],
        jq_schema: Optional[str] = ".",
        content_key: Optional[str] = None,
        is_content_key_jq_parsable: Optional[bool] = False,
        metadata_func: Optional[Callable[[Dict, Dict], Dict]] = None,
        text_content: bool = True,
        json_lines: bool = False,
    ):
        """Initialize the JSONLoader."""
        try:
            import jq

            self.jq = jq
        except ImportError:
            raise ImportError(
                "jq package not found, please install it with `pip install jq`"
            )

        self.file_path = Path(file_path).resolve()
        self._jq_schema = jq.compile(jq_schema)
        self._is_content_key_jq_parsable = is_content_key_jq_parsable
        self._content_key = content_key
        self._metadata_func = metadata_func
        self._text_content = text_content
        self._json_lines = json_lines

    def load_and_process_json(self):
        """
        Load JSON file and process it into LangChain documents.
        """
        try:
            # Load JSON data
            with open(self.file_path, "r", encoding="utf-8") as f:
                data = json.load(f)

            # If data is a dictionary, convert it to a list
            if isinstance(data, dict):
                data = [data]

            logger.info(
                f"Successfully loaded JSON with {len(data)} items from {self.file_path}"
            )

            # Process each item in the data
            documents = []

            for item in data:
                # Convert the item to a JSON string for the document content
                content = json.dumps(item, indent=2)
                doc = Document(
                    page_content=content, metadata={"source": str(self.file_path)}
                )
                documents.append(doc)

            # Check if any documents are too large for embedding
            oversize_docs = [
                doc for doc in documents if len(doc.page_content) > MAX_TOKEN_LIMIT
            ]
            if oversize_docs:
                logger.warning(
                    f"WARNING: {len(oversize_docs)} documents exceed OpenAI's token limit "
                    "and may be truncated"
                )

            # No splitting - each JSON object stays as a single document
            split_docs = documents
            logger.info(f"Using {len(split_docs)} documents (one per JSON object)")

            return split_docs

        except Exception as e:
            logger.error(f"Error processing JSON file: {e}")
            raise

    def load(self) -> Iterator[Document]:
        """Load and return documents from the JSON file."""
        documents = self.load_and_process_json()
        for doc in documents:
            yield doc

    def lazy_load(self) -> Iterator[Document]:
        """Load and return documents lazily from the JSON file."""
        index = 0
        if self._json_lines:
            with self.file_path.open(encoding="utf-8") as f:
                for line in f:
                    line = line.strip()
                    if line:
                        for doc in self._parse(line, index):
                            yield doc
                            index += 1
        else:
            for doc in self._parse(self.file_path.read_text(encoding="utf-8"), index):
                yield doc
                index += 1

    def _parse(self, content: str, index: int) -> Iterator[Document]:
        """Convert given content to documents."""
        data = self._jq_schema.input(json.loads(content))

        # Perform some validation
        if self._content_key is not None:
            self._validate_content_key(data)
        if self._metadata_func is not None:
            self._validate_metadata_func(data)

        for i, sample in enumerate(data, index + 1):
            text = self._get_text(sample=sample)
            metadata = self._get_metadata(
                sample=sample, source=str(self.file_path), seq_num=i
            )
            yield Document(page_content=text, metadata=metadata)

    def _get_text(self, sample: Any) -> str:
        """Convert sample to string format"""
        if self._content_key is not None:
            if self._is_content_key_jq_parsable:
                compiled_content_key = self.jq.compile(self._content_key)
                content = compiled_content_key.input(sample).first()
            else:
                content = sample[self._content_key]
        else:
            content = sample

        if self._text_content and not isinstance(content, str):
            raise ValueError(
                f"Expected page_content is string, got {type(content)} instead. \
                    Set `text_content=False` if the desired input for \
                    `page_content` is not a string"
            )

        # In case the text is None, set it to an empty string
        elif isinstance(content, str):
            return content
        elif isinstance(content, dict):
            return json.dumps(content) if content else ""
        else:
            return str(content) if content is not None else ""

    def _get_metadata(
        self, sample: Dict[str, Any], **additional_fields: Any
    ) -> Dict[str, Any]:
        """
        Return a metadata dictionary based on the existence of metadata_func
        """
        if self._metadata_func is not None:
            return self._metadata_func(sample, additional_fields)
        else:
            return additional_fields

    def _validate_content_key(self, data: Any) -> None:
        """Check if a content key is valid"""

        sample = data.first()
        if not isinstance(sample, dict):
            raise ValueError(
                f"Expected the jq schema to result in a list of objects (dict), \
                    so sample must be a dict but got `{type(sample)}`"
            )

        if (
            not self._is_content_key_jq_parsable
            and sample.get(self._content_key) is None
        ):
            raise ValueError(
                f"Expected the jq schema to result in a list of objects (dict) \
                    with the key `{self._content_key}`"
            )
        if (
            self._is_content_key_jq_parsable
            and self.jq.compile(self._content_key).input(sample).text() is None
        ):
            raise ValueError(
                f"Expected the jq schema to result in a list of objects (dict) \
                    with the key `{self._content_key}` which should be parsable by jq"
            )

    def _validate_metadata_func(self, data: Any) -> None:
        """Check if the metadata_func output is valid"""

        sample = data.first()
        if self._metadata_func is not None:
            sample_metadata = self._metadata_func(sample, {})
            if not isinstance(sample_metadata, dict):
                raise ValueError(
                    f"Expected the metadata_func to return a dict but got \
                        `{type(sample_metadata)}`"
                )


CHUNK_SIZE = 2000
CHUNK_OVERLAP = 200
# Registry of available loaders
LOADER_REGISTRY: Dict[str, Type[BaseLoader]] = {
    "csv": CSVLoader,
    "pdf": PyMuPDFLoader,
    "docx": Docx2txtLoader,
    "md": UnstructuredMarkdownLoader,
    "txt": TextLoader,
    "json": CustomJSONLoader,
    # Add more loaders as needed
}
