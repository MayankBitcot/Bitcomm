"""
Core chat model functionality.
"""
from langchain_community.chat_models import ChatLiteLLM
from langchain_core.callbacks import CallbackManager
from langchain_core.messages import HumanMessage

from ragent.log_utils import logger
from ragent.openai_constants import OpenAIConfig


def get_chat_litellm(
    model: str,
    api_key: str = OpenAIConfig.API_KEY,
    max_retries: int = 3,
    streaming: bool = True,
    callbacks=None,
) -> ChatLiteLLM:
    """
    Get a configured ChatLiteLLM instance.

    Args:
        model: The name of the model to use.
        api_key: The API key to use. Defaults to OpenAIConfig.API_KEY.
        max_retries: The maximum number of retries.
        streaming: Whether to use streaming.
        callbacks: The callbacks to use.

    Returns:
        A configured ChatLiteLLM instance.
    """
    logger.info("Initializing ChatLiteLLM...")
    return ChatLiteLLM(
        model=model,
        api_key=api_key,
        max_retries=max_retries,
        streaming=streaming,
        callback_manager=CallbackManager(callbacks) if callbacks else None,
    )


def validate_model(model_name: str, api_key: str = OpenAIConfig.API_KEY) -> bool:
    """
    Validate if a model can be used successfully.

    Args:
        model_name: The name of the model to validate.
        api_key: The API key to use. Defaults to OpenAIConfig.API_KEY.

    Returns:
        True if the model is valid, False otherwise.
    """
    logger.info("Validating model...")
    chat_llm = ChatLiteLLM(model=model_name, api_key=api_key)
    test_message = HumanMessage(content="What model are you?")
    response = chat_llm.invoke([test_message])
    return response
