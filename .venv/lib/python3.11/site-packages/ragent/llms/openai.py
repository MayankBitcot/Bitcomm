from typing import Any, Dict, List, Optional, Union

from langchain_core.callbacks import BaseCallbackHandler
from langchain_core.messages import HumanMessage
from langchain_openai import ChatOpenAI, OpenAIEmbeddings

from ragent.log_utils import logger
from ragent.openai_constants import OpenAIConfig


class OpenAIChat:
    """
    Core class for interacting with OpenAI's chat models using LangChain.
    """

    def __init__(
        self,
        model_name: str = OpenAIConfig.CHAT_MODEL_NAME,
        temperature: float = 0,
        max_tokens: int = OpenAIConfig.MAX_TOKENS,
        max_retries: int = OpenAIConfig.MAX_RETRIES,
    ):
        """
        Initialize the OpenAI chat client.

        Args:
            model_name: Model to use (default: gpt-4o-mini)
            temperature: Controls randomness (0=deterministic, 1=creative)
            max_tokens: Maximum number of tokens in the response
            max_retries: Maximum number of retries on API failure
        """
        self.model_name = model_name
        self.temperature = temperature
        self.max_tokens = max_tokens
        self.max_retries = max_retries

    def get_client(
        self,
        streaming: bool = False,
        callbacks: Optional[List[BaseCallbackHandler]] = None,
    ) -> ChatOpenAI:
        """
        Get configured ChatOpenAI client.

        Args:
            streaming: Whether to stream the response
            callbacks: List of callback handlers

        Returns:
            Configured ChatOpenAI client
        """
        client = ChatOpenAI(
            model=self.model_name,
            temperature=self.temperature,
            openai_api_key=OpenAIConfig.API_KEY,
            max_tokens=self.max_tokens,
            max_retries=self.max_retries,
            streaming=streaming,
            callbacks=callbacks,
        )
        logger.info("OpenAI chat client initialization")
        return client

    def get_completion(self, prompt: str) -> str:
        """
        Get a simple completion from the model.

        Args:
            prompt: Text prompt to send to the model

        Returns:
            Model's response as a string
        """
        client = self.get_client()
        message = HumanMessage(content=prompt)
        response = client.invoke([message])
        return response.content

    def get_structured_output(
        self, prompt: str, output_schema: str
    ) -> Union[Dict, Any]:
        """
        Get structured output from the model based on a schema.

        Args:
            prompt: Text prompt to send to the model
            output_schema: Pydantic model or schema for structured output

        Returns:
            Structured data conforming to the provided schema
        """
        client = self.get_client()
        structured_llm = client.with_structured_output(
            output_schema, method="json_mode"
        )
        message = HumanMessage(content=prompt)
        return structured_llm.invoke([message])


class OpenAIEmbeddingClient:
    """
    Core class for generating embeddings using OpenAI's embedding models.
    """

    def __init__(
        self,
        max_retries: int = OpenAIConfig.MAX_RETRIES,
        model_name: str = OpenAIConfig.EMBEDDING_MODEL_NAME,
    ):
        """
        Initialize the OpenAI embeddings client.

        Args:
            api_key: OpenAI API key
            max_retries: Maximum number of retries on API failure
            model_name: Embedding model to use
        """
        self.api_key = OpenAIConfig.API_KEY
        self.max_retries = max_retries
        self.model_name = model_name

    def get_embeddings_client(self) -> OpenAIEmbeddings:
        """
        Get configured OpenAIEmbeddings client.

        Returns:
            Configured OpenAIEmbeddings client
        """
        client = OpenAIEmbeddings(
            openai_api_key=OpenAIConfig.API_KEY,
            max_retries=self.max_retries,
            model=self.model_name,
        )
        logger.info("OpenAI embeddings client initialization")
        return client


class OpenAILLMProvider:
    def __init__(
        self,
        model_name: str,
        max_tokens: int,
        openai_api_key: str,
        temperature: float = 0,
        max_retries: int = 3,
    ) -> None:
        super().__init__()  # Initialize the base class
        self.model_name: str = model_name
        self.openai_api_key: str = openai_api_key
        self.max_tokens: int = max_tokens
        self.temperature = temperature
        self.max_retries = max_retries
        self.openai_client = OpenAIChat(
            api_key=openai_api_key,
            model_name=model_name,
            temperature=temperature,
            max_tokens=max_tokens,
            max_retries=max_retries,
        )

    def configure_model(self, streaming: bool = False, callbacks: list = None):
        model = self.openai_client.get_client(streaming=streaming, callbacks=callbacks)

        return model
