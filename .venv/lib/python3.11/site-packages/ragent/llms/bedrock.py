from typing import Any, List, Optional

from langchain_community.chat_models import BedrockChat
from langchain_community.llms.bedrock import Bedrock

# Core functions for LLM providers


def get_bedrock_text_model(
    model_id: str,
    client: Any,
    max_tokens: int = 1024,
    temperature: float = 0,
    streaming: bool = False,
    callbacks: Optional[List[Any]] = None,
) -> Bedrock:
    """
    Get a configured Bedrock text completion model.

    Args:
        model_id: The ID of the model to use.
        client: The Bedrock client.
        max_tokens: The maximum number of tokens to generate.
        temperature: The temperature to use for generation.
        streaming: Whether to use streaming.
        callbacks: The callbacks to use.

    Returns:
        A configured Bedrock instance.
    """
    return Bedrock(
        model_id=model_id,
        client=client,
        streaming=streaming,
        callbacks=callbacks,
        model_kwargs={
            "maxTokenCount": max_tokens,
            "temperature": temperature,
        },
    )


def get_bedrock_chat_model(
    model_id: str,
    client: Any,
    max_tokens: int = 1024,
    temperature: float = 0,
    streaming: bool = False,
    callbacks: Optional[List[Any]] = None,
) -> BedrockChat:
    """
    Get a configured BedrockChat instance.

    Args:
        model_id: The ID of the model to use.
        client: The Bedrock client.
        max_tokens: The maximum number of tokens to generate.
        temperature: The temperature to use for generation.
        streaming: Whether to use streaming.
        callbacks: The callbacks to use.

    Returns:
        A configured BedrockChat instance.
    """
    return BedrockChat(
        model_id=model_id,
        client=client,
        streaming=streaming,
        callbacks=callbacks,
        model_kwargs={
            "max_tokens_to_sample": max_tokens,
            "temperature": temperature,
        },
    )


class BedRockLLMProvider:
    def __init__(
        self, client, model_name: str, max_tokens: int, temperature: float = 0
    ) -> None:
        super().__init__()
        self.client = client
        self.model_name = model_name
        self.max_tokens = max_tokens
        self.temperature = temperature

    def configure_text_model(
        self, streaming: bool = False, callbacks: Optional[List[Any]] = None
    ) -> Any:
        return get_bedrock_text_model(
            model_id=self.model_name,
            client=self.client,
            max_tokens=self.max_tokens,
            temperature=self.temperature,
            streaming=streaming,
            callbacks=callbacks,
        )

    def configure_chat_model(
        self, streaming: bool = False, callbacks: Optional[List[Any]] = None
    ) -> Any:
        return get_bedrock_chat_model(
            model_id=self.model_name,
            client=self.client,
            max_tokens=self.max_tokens,
            temperature=self.temperature,
            streaming=streaming,
            callbacks=callbacks,
        )
