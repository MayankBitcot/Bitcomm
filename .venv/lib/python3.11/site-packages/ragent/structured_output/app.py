from typing import Type, Union

from langchain_core.messages import HumanMessage
from pydantic import BaseModel

from ragent.llms.openai import OpenAIChat
from ragent.loader.app import load_document
from ragent.log_utils import logger
from ragent.openai_constants import OpenAIConfig
from ragent.structured_output.prompt import JSON_PROMPT


class LLMConfig(BaseModel):
    """
    Configuration for LLM API call.

    Attributes:
        api_key (str): API key for the LLM.
        model (str): Model to use for the LLM.
        temperature (float, optional): Temperature for the LLM. Defaults to 0.
        max_tokens (int, optional): Maximum number of tokens for the LLM. Defaults to 2000.
    """

    api_key: str
    model: str
    temperature: float = 0
    max_tokens: int = 2000


class PDFProcessor:
    """
    Class to handle PDF text extraction using LangChain's PyMuPDFLoader.

    Attributes:
        pdf_path (str): The path to the PDF file to be processed.
    """

    def __init__(self, pdf_path: str) -> None:
        """
        Initializes the PDFProcessor object with a PDF file path.

        Args:
            pdf_path (str): The path to the PDF file to be processed.
        """
        self.pdf_path = pdf_path

    def extract_text(self) -> str:
        """
        Extracts text from a PDF file using PyMuPDFLoader.

        Returns:
            str: The extracted text from the PDF file.

        Raises:
            Exception: If an error occurs during text extraction.
        """
        try:
            logger.info(f"Extracting text from {self.pdf_path}")

            documents = load_document(file_path=self.pdf_path)
            logger.info("Documents loaded successfully")

            text = ""
            for doc in documents:
                text += doc.page_content + "\n"

            logger.info(f"Successfully extracted {len(text)} characters from PDF")
            return text

        except Exception as e:
            logger.error(f"Error extracting text from PDF: {str(e)}")
            raise


class JsonGenerator:
    """
    Class to generate JSON based on Pydantic models using LangChain's ChatOpenAI.

    Attributes:
        llm_config (LLMConfig): Configuration for the LLM.
        openai_client (OpenAIChat): OpenAI client instance.
    """

    def __init__(self, llm_config: LLMConfig) -> None:
        """
        Initializes the JsonGenerator object with an LLMConfig object and
        sets up an OpenAI client with the provided configuration.

        Args:
            llm_config (LLMConfig): Configuration for the LLM.
        """
        self.llm_config = llm_config
        self.openai_client = OpenAIChat(
            model_name=llm_config.model,
            temperature=llm_config.temperature,
            max_tokens=llm_config.max_tokens,
        )

    def generate_json(
        self,
        schema: Union[Type[BaseModel], str, dict],
        extracted_text: str,
        prompt: str,
    ) -> str:
        """
        Generates JSON from a schema definition using ChatOpenAI.

        Args:
            schema (Union[Type[BaseModel], str, dict]): Schema definition for structuring
                the output. Can be a Pydantic class, JSON schema string, or dictionary.
            extracted_text (str): Text from which to extract structured data.
            prompt (str): Prompt to use for generating JSON.

        Returns:
            str: JSON data as a string.

        Raises:
            Exception: If an error occurs while calling the OpenAI client.
        """
        try:
            client = self.openai_client.get_client()
            logger.info("Calling OpenAI client to generate JSON")

            UPDATED_JSON_PROMPT = prompt.format(
                schema=schema,
                extracted_text=extracted_text,
            )

            message = HumanMessage(content=UPDATED_JSON_PROMPT)
            response = client.invoke([message])

            generated_text = response.content
            cleaned_text = generated_text.strip()

            # Remove markdown JSON code block if present
            if cleaned_text.startswith("```json"):
                cleaned_text = cleaned_text.replace("```json", "", 1)
                if cleaned_text.endswith("```"):
                    cleaned_text = cleaned_text[:-3]
            elif cleaned_text.startswith("```"):
                cleaned_text = cleaned_text.replace("```", "", 1)
                if cleaned_text.endswith("```"):
                    cleaned_text = cleaned_text[:-3]

            cleaned_text = cleaned_text.strip()
            logger.info(f"cleaned text: {cleaned_text}")

            return cleaned_text

        except Exception as e:
            logger.error(f"Error calling OpenAI client: {str(e)}")
            raise


def text_to_json(
    text: str,
    schema: Union[Type[BaseModel], str, dict],
    prompt: str = JSON_PROMPT,
) -> str:
    """
    Converts plain text to JSON data using OpenAI's Large Language Model (LLM)
    and a schema definition for structuring the output.

    Args:
        text (str): The text to be converted to JSON.
        schema (Union[Type[BaseModel], str, dict], optional): The schema definition used to
            structure the output JSON. Can be a Pydantic class, a JSON schema string,
            or a dictionary representing the schema. Defaults to ResumeData.
        prompt (str, optional): The prompt used for the LLM. Defaults to JSON_PROMPT.

    Returns:
        str: The generated JSON data as a string.

    Raises:
        Exception: If the JSON generation fails.
    """
    # Create LLM config
    llm_config = LLMConfig(
        api_key=OpenAIConfig.API_KEY,
        model=OpenAIConfig.CHAT_MODEL_NAME,
    )

    logger.info(f"Processing plain text input of length: {len(text)}")

    generator = JsonGenerator(llm_config)
    json_output = generator.generate_json(schema, text, prompt)
    logger.info(f"Generated JSON: {json_output}")
    logger.info("Successfully generated JSON from Pydantic class")

    return json_output


def file_to_json(
    file_path: str,
    schema: Union[Type[BaseModel], str, dict],
    prompt: str = JSON_PROMPT,
) -> str:
    """
    Converts a file to JSON data using OpenAI's Large Language Model (LLM)
    and a schema definition for structuring the output.

    Args:
        file_path (str): The file path of the document to be converted.
        schema (Union[Type[BaseModel], str, dict], optional): The schema definition used to
            structure the output JSON. Can be a Pydantic class, a JSON schema string,
            or a dictionary representing the schema. Defaults to ResumeData.
        prompt (str, optional): The prompt used for the LLM. Defaults to JSON_PROMPT.

    Returns:
        str: The generated JSON data as a string.

    Raises:
        Exception: If the file cannot be processed or the JSON generation fails.
    """
    logger.info(f"File path: {file_path}")

    # Extract text from file
    pdf_processor = PDFProcessor(file_path)
    extracted_text = pdf_processor.extract_text()
    logger.info(f"Extracted text length: {len(extracted_text)}")

    # Generate JSON from the extracted text
    return text_to_json(text=extracted_text, schema=schema, prompt=prompt)


def process_json(
    input_source: Union[str, dict],
    is_file_path: bool,
    schema: Union[Type[BaseModel], str, dict],
    prompt: str = JSON_PROMPT,
) -> str:
    """
    Processes either a file path or plain text to generate JSON data using OpenAI's LLM
    and a schema definition for structuring the output.

    Args:
        input_source (Union[str, dict]): Either a file path or plain text content.
        is_file_path (bool, optional): Flag indicating if input_source is a file path.
        schema (Union[Type[BaseModel], str, dict], optional): The schema definition used to
            structure the output JSON. Can be a Pydantic class, a JSON schema string,
            or a dictionary representing the schema.
        prompt (str, optional): The prompt used for the LLM. Defaults to JSON_PROMPT.

    Returns:
        str: The generated JSON data as a string.

    Raises:
        Exception: If the input cannot be processed or the JSON generation fails.
    """
    if is_file_path:
        return file_to_json(
            file_path=input_source,
            schema=schema,
            prompt=prompt,
        )
    else:
        return text_to_json(
            text=input_source,
            schema=schema,
            prompt=prompt,
        )
