"""
Voice Form Service for audio-to-form transcription and parsing.
"""

import json
import logging
from pathlib import Path
from typing import Any, BinaryIO, Dict, Optional, Union

from openai import OpenAI

from ragent.openai_constants import OpenAIConfig
from ragent.transcribe.base import BaseTranscriber
from ragent.transcribe.openai import (
    AudioValidationError,
    HallucinationDetectedError,
    WhisperAPIError,
    WhisperTranscriber,
)

from .config import AudioLLMConfig, TextLLMConfig
from .exceptions import AudioProcessingError, TextParsingError, ValidationError

logger = logging.getLogger(__name__)


class VoiceFormService:
    """
    Service for converting audio to structured form data.

    Args:
        audio_config: Configuration for Whisper transcription
        api_key: OpenAI API key
        text_config: Configuration for GPT parsing (optional if form_fields provided per request)
        hallucination_check: Enable hallucination detection (default: True)
    """

    def __init__(
        self,
        api_key: Optional[str] = None,
        audio_config: Optional[AudioLLMConfig] = None,
        text_config: Optional[TextLLMConfig] = None,
        hallucination_check: bool = True,
        transcriber: Optional[BaseTranscriber] = None,
    ):
        if audio_config is None:
            raise ValueError("audio_config must be provided")

        resolved_api_key = api_key or OpenAIConfig.API_KEY
        if not resolved_api_key:
            raise ValueError(
                "OpenAI API key must be provided either explicitly or via secrets manager"
            )

        self.api_key = resolved_api_key
        self.audio_config = audio_config
        self.text_config = text_config

        # Initialize transcriber
        self.transcriber = transcriber or WhisperTranscriber(
            api_key=self.api_key,
            hallucination_check_enabled=hallucination_check,
        )

        # Initialize OpenAI client for GPT
        self.openai_client = OpenAI(api_key=self.api_key)

    def process_audio(
        self,
        audio_input: Union[str, Path, bytes, BinaryIO],
        filename: Optional[str] = None,
        content_type: Optional[str] = None,
        form_fields: Optional[Dict[str, str]] = None,
        validate_form_fields: Optional[bool] = None,
    ) -> Dict[str, Any]:
        """
        Process audio and extract form fields.

        Args:
            audio_input: Audio data (bytes, file path, or file-like object)
            filename: Filename for format detection
            content_type: MIME type of audio
            form_fields: Optional dynamic form schema (overrides text_config.form_schema)
            validate_form_fields: Whether to compute/enforce missing mandatory field validation

        Returns:
            Dict containing:
                - Extracted form fields
                - raw_transcription: Original transcribed text
                - missing_fields: List of fields that were not extracted

        Raises:
            AudioProcessingError: If transcription fails
            TextParsingError: If parsing fails
            ValidationError: If neither form_fields nor text_config provided
        """
        # Determine which text config to use
        if form_fields:
            # Create dynamic text config from form_fields
            text_config = TextLLMConfig(
                model=self.text_config.model if self.text_config else "gpt-4o-mini",
                temperature=self.text_config.temperature if self.text_config else 0,
                form_schema=form_fields,
            )
        elif self.text_config:
            text_config = self.text_config
        else:
            raise ValidationError("Either form_fields or text_config must be provided")

        # Step 1: Transcribe audio
        raw_transcription = self._transcribe_audio(audio_input, filename, content_type)

        # Step 2: Parse transcription into structured fields
        extracted_fields = self._parse_transcription(raw_transcription, text_config)

        # Step 3: Check for missing fields
        should_validate = (
            bool(validate_form_fields) if validate_form_fields is not None else False
        )
        if should_validate:
            logger.info(
                "Voice form filler: validate_form_fields enabled; computing missing fields"
            )
            missing_fields = self._check_missing_fields(extracted_fields, text_config)
        else:
            logger.info(
                "Voice form filler: validate_form_fields disabled; skipping missing field validation"
            )
            missing_fields = []

        # Step 4: Build response
        result = {
            **extracted_fields,
            "raw_transcription": raw_transcription,
            "missing_fields": missing_fields,
        }

        return result

    def _transcribe_audio(
        self,
        audio_input: Union[str, Path, bytes, BinaryIO],
        filename: Optional[str],
        content_type: Optional[str],
    ) -> str:
        """
        Transcribe audio using Whisper.

        Returns:
            str: Transcribed text

        Raises:
            AudioProcessingError: If transcription fails
        """
        try:
            result = self.transcriber.transcribe(
                audio_input=audio_input,
                filename=filename,
                language=self.audio_config.language,
                prompt=self.audio_config.prompt,
                temperature=self.audio_config.temperature,
            )

            return result["transcript"]

        except AudioValidationError as e:
            raise AudioProcessingError(f"Invalid audio file: {str(e)}")
        except HallucinationDetectedError as e:
            raise AudioProcessingError(f"Transcription quality issue: {str(e)}")
        except WhisperAPIError as e:
            raise AudioProcessingError(f"Transcription service error: {str(e)}")
        except Exception as e:
            raise AudioProcessingError(f"Audio processing failed: {str(e)}")

    def _parse_transcription(
        self, transcription: str, text_config: TextLLMConfig
    ) -> Dict[str, str]:
        """
        Parse transcription into structured fields using GPT.

        Args:
            transcription: Raw transcribed text
            text_config: Text configuration to use

        Returns:
            Dict of extracted fields

        Raises:
            TextParsingError: If parsing fails
        """
        try:
            # Build the prompt
            prompt = text_config.build_prompt()

            # Call GPT
            response = self.openai_client.chat.completions.create(
                model=text_config.model,
                messages=[
                    {"role": "system", "content": prompt},
                    {"role": "user", "content": transcription},
                ],
                temperature=text_config.temperature,
            )

            # Extract and parse JSON
            parsed_data = response.choices[0].message.content
            form_data = json.loads(parsed_data)

            return form_data

        except json.JSONDecodeError as e:
            raise TextParsingError(f"Failed to parse GPT response as JSON: {str(e)}")
        except Exception as e:
            raise TextParsingError(f"Text parsing failed: {str(e)}")

    def _check_missing_fields(
        self, extracted_fields: Dict[str, str], text_config: TextLLMConfig
    ) -> list:
        """
        Check which fields are missing or empty.

        Args:
            extracted_fields: Extracted form fields
            text_config: Text configuration used

        Returns:
            List of missing field names
        """
        missing = []

        # Get expected fields from form_schema
        if text_config.form_schema:
            for field_name in text_config.form_schema.keys():
                value = extracted_fields.get(field_name, "")
                if not value or value.strip() == "":
                    missing.append(field_name)

        return missing
