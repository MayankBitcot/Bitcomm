"""
Configuration classes for Audio and Text LLM processing.
"""

import logging
from typing import Dict, Optional

from .exceptions import ValidationError

logger = logging.getLogger(__name__)


class AudioLLMConfig:
    """
    Configuration for audio transcription (Whisper).

    Args:
        model: Whisper model name (default: "whisper-1")
        language: Language code for transcription (default: "en")
        temperature: Sampling temperature (default: 0.0)
        prompt: Custom prompt for transcription context (default: generic symbols prompt)
    """

    def __init__(
        self,
        model: str = "whisper-1",
        language: str = "en",
        temperature: float = 0.0,
        prompt: Optional[str] = None,
    ):
        self.model = model
        self.language = language
        self.temperature = temperature
        self.prompt = prompt or "Symbols: 'at' or 'at the rate' means @, 'dot' means ."


class TextLLMConfig:
    """
    Configuration for text parsing (GPT).

    Args:
        model: GPT model name (default: "gpt-4o-mini")
        temperature: Sampling temperature (default: 0)
        form_schema: Dict of field names to descriptions (auto-generates prompt)
        custom_prompt: Custom prompt for extraction (overrides auto-generation)

    Note: Provide either form_schema OR custom_prompt, not both.
    """

    def __init__(
        self,
        model: str = "gpt-4o-mini",
        temperature: int = 0,
        form_schema: Optional[Dict[str, str]] = None,
        custom_prompt: Optional[str] = None,
    ):
        if form_schema and custom_prompt:
            raise ValidationError(
                "Provide either 'form_schema' or 'custom_prompt', not both"
            )

        if not form_schema and not custom_prompt:
            raise ValidationError(
                "Must provide either 'form_schema' or 'custom_prompt'"
            )

        self.model = model
        self.temperature = temperature
        self.form_schema = form_schema
        self.custom_prompt = custom_prompt

    def build_prompt(self) -> str:
        """
        Build the extraction prompt based on configuration.

        Returns:
            str: The extraction prompt for GPT
        """
        if self.custom_prompt:
            return self.custom_prompt

        if self.form_schema:
            # Build field descriptions
            logger.info("=================input schema==================")
            logger.info(f"{self.form_schema}")
            logger.info("=====================================")
            fields_json = "{\n"
            for key, description in self.form_schema.items():
                # Use description if provided, otherwise use field name
                desc = description if description and description.strip() else key
                fields_json += f'    "{key}": "extracted {desc} or empty string",\n'
            fields_json = fields_json.rstrip(",\n") + "\n}"

            # Build extraction instructions
            field_names = ", ".join(self.form_schema.keys())

            prompt = f"""Extract {field_names} from the given text.
Return ONLY valid JSON in this exact format:
{fields_json}"""

            logger.info("=== PROMPT SENT TO GPT ===")
            logger.info(f"{prompt}")
            logger.info("========================")
            return prompt

        raise ValidationError("No form_schema or custom_prompt provided")
