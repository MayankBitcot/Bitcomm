from io import StringIO
from typing import Any, List, Optional, Tuple

from langchain_classic.chains.combine_documents import create_stuff_documents_chain
from langchain_classic.chains.llm import LLMChain
from langchain_classic.chains.qa_with_sources.retrieval import (
    RetrievalQAWithSourcesChain,
)
from langchain_core.documents import Document
from langchain_core.messages import SystemMessage
from langchain_core.prompts import (
    ChatPromptTemplate,
    HumanMessagePromptTemplate,
    PromptTemplate,
)

from ragent.log_utils import logger


def create_retrieval_qa_chain(
    llm: Any,
    retriever: Any,
    prompt_template: Any,
) -> RetrievalQAWithSourcesChain:
    """
    Create a RetrievalQAWithSourcesChain using the provided LLM, retriever, and prompt template.

    Args:
        llm: The language model to use
        retriever: The retriever to use for fetching documents
        prompt_template: The prompt template to use for the chain

    Returns:
        RetrievalQAWithSourcesChain: The chain for question answering with sources
    """
    chain_type_kwargs = {"prompt": prompt_template}

    chain = RetrievalQAWithSourcesChain.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=retriever,
        chain_type_kwargs=chain_type_kwargs,
        return_source_documents=True,
    )

    return chain


async def generate_rag_response(
    chain: RetrievalQAWithSourcesChain,
    user_input: str,
    log_capture_string: Optional[StringIO] = None,
    fallback_response: str = "",
) -> Tuple[str, List[Any], Optional[str], bool]:
    """
    Generate a response using a RetrievalQAWithSourcesChain.

    Args:
        chain: The RetrievalQAWithSourcesChain to use
        user_input: The user's question
        log_capture_string: StringIO object for capturing logs
        fallback_response: Response that indicates a fallback answer was provided

    Returns:
        Tuple containing (response text, source documents, multiple queries,
        skip_related_questions flag)
    """
    # Generate response using the chain
    chain_response = chain({"question": user_input})
    logger.info(f"chain response: {chain_response}")

    response = chain_response["answer"]
    source_documents = chain_response["source_documents"]
    logger.info(f"source document: {source_documents}, response: {response}")

    # Capture generated queries if log_capture_string is provided
    multiple_queries = None
    if log_capture_string:
        multiple_queries = log_capture_string.getvalue()
        # Clear the capture string after saving the logs
        log_capture_string.truncate(0)
        log_capture_string.seek(0)

    # Determine if we should skip related questions (if fallback response is provided)
    skip_related_questions = response.strip() == fallback_response

    logger.info(
        f"skip_related_questions: {skip_related_questions}, multiple_queries: {multiple_queries}"
    )

    return response, source_documents, multiple_queries, skip_related_questions


async def generate_related_questions(
    llm: Any,
    source_documents: List[Any],
    user_input: str,
    prompt: str,
    context: str = "",
    no_of_questions: int = None,
) -> str:
    """
    Generate related questions based on the user input and source documents.

    Args:
        llm: The language model to use
        source_documents: Retrieved documents to use as context
        user_input: The user's question
        prompt: The prompt to use for generating related questions

    Returns:
        String containing generated related questions
    """
    logger.info("Starting to generate related questions")
    logger.info(f"User input: {user_input}")
    logger.info(f"Context provided: {'Yes' if context else 'No'}")
    logger.info(f"Number of questions to generate: {no_of_questions}")

    # Use provided context if available, otherwise build from source_documents
    if not context and source_documents:
        context = "\n\n".join([doc.page_content for doc in source_documents])

    # Create proper chat prompt template
    logger.info("Creating chat prompt template")
    try:
        chat_prompt = ChatPromptTemplate.from_messages(
            [
                SystemMessage(content=prompt),
                HumanMessagePromptTemplate.from_template(
                    """<question>{question}</question>
<context>{context}</context>
<no_of_questions>{no_of_questions}</no_of_questions>"""
                ),
            ]
        )
    except Exception as e:
        logger.error(f"Error creating chat prompt template: {str(e)}")
        raise

    # Create the chain with the properly formatted prompt
    logger.info("Creating LLMChain")
    try:
        chain = LLMChain(llm=llm, prompt=chat_prompt)
    except Exception as e:
        logger.error(f"Error creating LLMChain: {str(e)}")
        raise

    # Generate related questions
    logger.info("Invoking chain to generate related questions")
    try:
        chain_response = chain.invoke(
            {
                "question": user_input,
                "context": context,
                "no_of_questions": no_of_questions,
            }
        )
        logger.info(f"Related question chain response: {chain_response}")
    except Exception as e:
        logger.error(f"Error invoking chain: {str(e)}")
        raise

    response = chain_response["text"]
    logger.info(f"Related question response: {response}")

    cleaned_response = response.lower().strip()
    if (
        "fail" in cleaned_response
        or cleaned_response.replace(" ", "")
        == "<related_question>fail</related_question>"
    ):
        return ""

    return response


async def generate_standard_response(
    llm: Any,
    prompt_template: Any,
    user_input: str,
) -> Tuple[str, List[Any]]:
    """
    Generate a response using a standard LLMChain without retrieval.

    Args:
        llm: The language model to use
        prompt_template: The prompt template to use
        user_input: The user's question

    Returns:
        Tuple containing response text and empty source documents list
    """
    chain = LLMChain(llm=llm, prompt=prompt_template)
    chain_response = chain.invoke({"question": user_input})
    logger.debug(f"COMPLETE RESPONSE {chain_response}")

    response = chain_response["text"]
    logger.debug(f"response........{response}")
    source_documents = []

    return response, source_documents


def create_stuff_chain(
    llm: Any,
    prompt: PromptTemplate,
    user_input: str,
    chat_history_str: str,
    docs: List[Document],
) -> Any:
    """
    Create a stuff documents chain that combines documents into a single context.

    Args:
        llm: The language model to use
        prompt: The prompt template to use
        user_input: The user's query
        chat_history_str: The chat history as a string
        docs: The documents to combine

    Returns:
        The response from the chain
    """

    chain = create_stuff_documents_chain(
        llm=llm,
        prompt=prompt,
        document_prompt=PromptTemplate.from_template("{page_content}"),
        document_variable_name="content",
    )
    logger.info(f"Sending question to LLM: {user_input}")
    logger.info(f"Chat history entries: {len(chat_history_str)}")

    response = chain.ainvoke(
        {
            "question": user_input,
            "chat_history": chat_history_str,
            "content": docs,
        }
    )

    return response
